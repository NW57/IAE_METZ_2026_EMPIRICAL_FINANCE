{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "from scipy.stats import norm, jarque_bera\n",
    "\n",
    "def filtrage_ETF(etf_ticker_placeholder):\n",
    "    # Placeholder tickers\n",
    "    placeholder_tickers = list(set([\n",
    "        'NVDA','AAPL','MSFT','AMZN','GOOGL','GOOG','META','AVGO','TSLA','LLY','JPM','UNH','XOM','V','MA','COST','ORCL','HD','PG',\n",
    "        'NFLX','ABBV','BAC','SAP','ASML','CRM','WMT','KO','CVX','TM','ADBE','PEP','MRK','LIN','TMO','WFC','MCD','ACN','CSCO','ABT',\n",
    "        'DIS','QCOM','GE','INTU','DHR','CAT','VZ','TXN','IBM','AMAT','AXP','PFE','MS','PM','AMGN','GS','ISRG','NOW','HON','RTX','LOW',\n",
    "        'BKNG','SPGI','INTC','SYK','C','TJX','PGR','BLK','ELV','REGN','SCHW','VRTX','ETN','BSX','LMT','ADI','PANW','MDLZ','CB','CI',\n",
    "        'MMC','BA','MU','PLTR','GILD','ADP','LRCX','DE','VRT','T','WM','AMT','MDT','ZTS','ICE','SBUX','MO','ITW','SHW','ANET','CVS',\n",
    "        'HCA','BX','EQIX','ECL','PH','GD','MCK','ORLY','CDNS','MAR','NXPI','CTAS','BDX','APH','MCO','TGT','F','EMR','ADSK','ROP','TDG',\n",
    "        'KLAC','AON','CMG','PNC','TEL','MET','CARR','SNPS','EW','DASH','APO','DLR','AJG','MCHP','COF','MSI','GWW','TRV','HUM','AIG',\n",
    "        'ALL','CPRT','USB','O','STZ','MPC','KMB','VRSK','AZO','FMS','NSC','NEM','GLW','PAYX','GEV','IT','KMI','KDP','WMB','BKR','WELL',\n",
    "        'DOW','A','PRU','STT','OTIS','FAST','KR','CEG','ODFL','CTVA','ED','VICI','BBY','EIX','HPQ','DD','HAL','VLO','OKE','TRGP','MTD',\n",
    "        'KEYS','K','FITB','EFX','EBAY','WST','WDC','ROK','GPN','FICO','ZBRA','TER','ALGN','VTR','AVB','EQR','ARE','INVH','SBAC','EXR',\n",
    "        'MAA','CPT','BXP','UDR','DOC','REG','KIM','FRT','HST','STAG','NNN','WPC','ADC','PLD','SPG'\n",
    "    ]))\n",
    "\n",
    "    print(f\"Analyzing simulated holdings for ETF: {etf_ticker_placeholder}\")\n",
    "    print(f\"Number of tickers to analyze: {len(placeholder_tickers)}\")\n",
    "\n",
    "    endDate = dt.datetime.now()\n",
    "    startDate = endDate - dt.timedelta(days=365*5)\n",
    "\n",
    "    data = yf.download(\n",
    "        placeholder_tickers + ['GLD'],\n",
    "        start=startDate,\n",
    "        end=endDate,\n",
    "        auto_adjust=True,\n",
    "        threads=True,\n",
    "        progress=False\n",
    "    )['Close']\n",
    "\n",
    "    # Risk-free rate annual\n",
    "    rf = 0.03447\n",
    "\n",
    "        # Keep only tickers with at least 80% of historical data\n",
    "    min_data_points = int(len(data) * 0.8)\n",
    "    data = data.dropna(thresh=min_data_points, axis=1)\n",
    "    print(f\"Downloaded data for {data.shape[1]} tickers ({startDate.date()} to {endDate.date()})\")\n",
    "\n",
    "    # Daily log returns\n",
    "    log_returns = np.log(data / data.shift(1)).dropna()\n",
    "    print(f\"Calculated daily log returns. Shape: {log_returns.shape}\")\n",
    "\n",
    "    # Annualized returns and volatility\n",
    "    annual_mean_returns = log_returns.mean() * 252\n",
    "    annual_std_dev = log_returns.std() * np.sqrt(252)\n",
    "\n",
    "    # Sharpe ratios\n",
    "    sharpe_ratios = (annual_mean_returns - rf) / annual_std_dev\n",
    "    print(f\"Calculated Sharpe ratios for {len(sharpe_ratios)} tickers.\")\n",
    "\n",
    "    # Top 15 tickers by Sharpe ratio\n",
    "    top_15_actions = sharpe_ratios.drop('GLD').nlargest(15)\n",
    "    # Include GLD\n",
    "    top_15_pf = pd.concat([top_15_actions, pd.Series({'GLD': sharpe_ratios['GLD']})])\n",
    "    print(f\"Top 15 tickers + GLD for portfolio:\\n{top_15_pf}\")\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    top_15_pf.sort_values().plot(kind='barh', color='skyblue')\n",
    "    plt.title(f\"Top 15 Sharpe Ratios + Gold - {etf_ticker_placeholder}\")\n",
    "    plt.xlabel(\"Sharpe Ratio\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return top_15_pf, log_returns[top_15_pf.index]\n",
    "\n",
    "top_15_pf, returns_pf = filtrage_ETF(\"Amundi Core MSCI World\")\n",
    "\n",
    "print(\"Tickers sélectionnés pour le portefeuille :\")\n",
    "print(top_15_pf.index.tolist())\n",
    "\n",
    "print(\"--- Aperçu du Dataset Nettoyé (Rendements Log) ---\")\n",
    "print(returns_pf.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a345b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_prices = (returns_pf + 1).cumprod() * 100\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for ticker in cumulative_prices.columns:\n",
    "    plt.plot(cumulative_prices[ticker], label=ticker)\n",
    "plt.title(\"Évolution des prix base 100 des actifs retenus\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Prix cumulés (base 100)\")\n",
    "plt.legend(fontsize=8, ncol=3)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "centered_returns = returns_pf - returns_pf.mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "for ticker in centered_returns.columns:\n",
    "    plt.plot(centered_returns[ticker], alpha=0.7, label=ticker)\n",
    "plt.title(\"Visualisation des rendements logarithmiques\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rendement\")\n",
    "plt.legend(fontsize=8, ncol=3)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "corr_matrix = returns_pf.corr()\n",
    "heat_values = np.abs(corr_matrix)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(heat_values, annot=True, fmt=\".2f\", cmap=\"Reds\", cbar=True, square=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annualized volatility\n",
    "def standard_dev(weights, cov_matrix):\n",
    "  variance=weights.T @ cov_matrix @ weights #multiplying a matrix with weight ponderation\n",
    "  return np.sqrt(variance)\n",
    "\n",
    "  #Annualized return\n",
    "def expected_returns(weights,log_returns):\n",
    "  return np.sum(log_returns.mean()*weights)*252\n",
    "\n",
    "  #Sharpe Ratio\n",
    "def Sharpe_ratio(weights,log_returns,cov_matrix,rf):\n",
    "  return (expected_returns(weights,log_returns)-rf)/standard_dev(weights,cov_matrix)\n",
    "\n",
    "  #Value at Risk\n",
    "def plot_var_distribution(returns_dollar, VaR, confidence_interval, days, title):\n",
    "  plt.hist(returns_dollar.dropna(), bins=50, color='skyblue', edgecolor='black', density=True)\n",
    "  plt.xlabel(f'{days}- Day Portfolio Return (Dollar Value)')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(title)\n",
    "  plt.axvline(x=-VaR, color='red', linestyle='--', label=f'VaR {confidence_interval*100}%')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def Value_at_risk(returns_percentage, confidence_interval, portfolio_value):\n",
    "\n",
    "  return -np.percentile(returns_percentage, (1 - confidence_interval) * 100) * portfolio_value\n",
    "\n",
    "def VaR_parametric_constraint_func(weights, log_returns, confidence_interval, days):\n",
    "\n",
    "  portfolio_mean_daily_return = np.sum(log_returns.mean() * weights)\n",
    "\n",
    "  portfolio_daily_std_dev = np.sqrt(np.dot(weights.T, np.dot(log_returns.cov(), weights)))\n",
    "\n",
    "  alpha = 1 - confidence_interval\n",
    "  z_score = norm.ppf(alpha)\n",
    "\n",
    "  VaR_percent = -(portfolio_mean_daily_return * days + z_score * portfolio_daily_std_dev * np.sqrt(days))\n",
    "  return VaR_percent\n",
    "\n",
    "  #Max drawdown\n",
    "\n",
    "def summarize_max_drawdown(cumulative_returns):\n",
    "    running_max = cumulative_returns.expanding(min_periods=1).max()\n",
    "    drawdown = (cumulative_returns - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    print(f\"  Maximum Drawdown: {max_drawdown:.4f}\")\n",
    "    return max_drawdown\n",
    "\n",
    "    #Calculation of Skewness & Kurtosis\n",
    "\n",
    "def summarize_distribution_moments(optimal_returns, equal_returns):\n",
    "    print(\"Optimal Portfolio:\")\n",
    "    print(f\"  Skewness: {optimal_returns.skew():.4f}\")\n",
    "    print(f\"  Kurtosis: {optimal_returns.kurt():.4f}\")\n",
    "\n",
    "    print(\"\\nEqually Weighted Portfolio:\")\n",
    "    print(f\"  Skewness: {equal_returns.skew():.4f}\")\n",
    "    print(f\"  Kurtosis: {equal_returns.kurt():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_portfolio_analysis(tickers):\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    endDate = dt.datetime.now()\n",
    "    startDate = endDate - dt.timedelta(days=365 * 5)\n",
    "\n",
    "    # Fixed factors, updated to match IPS\n",
    "    rf = 0.03447\n",
    "    confidence_interval = 0.95\n",
    "    portfolio_value = 1000000\n",
    "    days = 5\n",
    "    num_simulations = 10000\n",
    "    num_days = 252\n",
    "\n",
    "    close_df = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        data = yf.download(ticker, start=startDate, end=endDate, progress=False, auto_adjust=False)\n",
    "        close_df[ticker] = data['Adj Close']\n",
    "\n",
    "    log_returns = np.log(close_df / close_df.shift(1)).dropna()\n",
    "\n",
    "    cov_matrix_opt = log_returns.cov() * 252\n",
    "\n",
    "    # Poids maximum par actif : 30 % -> upper bound 0.30\n",
    "    # Poids minimum par actif : 0% -> lower bound 0\n",
    "    bounds = [(0.0, 0.30) for _ in range(len(tickers))]\n",
    "\n",
    "    # Define constraints list\n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}, # Sum of weights = 1\n",
    "\n",
    "        # Volatility constraint: Annual Volatility <= 0.15\n",
    "        {'type': 'ineq', 'fun': lambda weights: 0.15 - standard_dev(weights, cov_matrix_opt)},\n",
    "\n",
    "        # Parametric VaR constraint: VaR (percentage) <= 0.10 (10%)\n",
    "        {'type': 'ineq', 'fun': lambda weights: 0.10 - VaR_parametric_constraint_func(weights, log_returns, confidence_interval, days)}\n",
    "    ]\n",
    "\n",
    "    # Initial weights\n",
    "    initial_weights = np.array([1 / len(tickers) for _ in range(len(tickers))])\n",
    "\n",
    "    # Optimize the weights\n",
    "    optimized_results = minimize(lambda weights: -Sharpe_ratio(weights, log_returns, cov_matrix_opt, rf),\n",
    "                                 initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    print(\"-- Optimal weights --\")\n",
    "\n",
    "    optimal_weights = optimized_results.x\n",
    "\n",
    "    optimal_portfolio_returns = expected_returns(optimal_weights, log_returns)\n",
    "    optimal_portfolio_std = standard_dev(optimal_weights, cov_matrix_opt)\n",
    "    optimal_sharpe_ratio = Sharpe_ratio(optimal_weights, log_returns, cov_matrix_opt, rf)\n",
    "\n",
    "    print(\"Optimal Weights:\")\n",
    "    for ticker, weight in zip(tickers, optimal_weights):\n",
    "        print(f\"{ticker}: {weight:.4f}\")\n",
    "\n",
    "    print(f\"\\nExpected Returns: {optimal_portfolio_returns:.4f}\")\n",
    "    print(f\"Standard Deviation: {optimal_portfolio_std:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {optimal_sharpe_ratio:.4f}\")\n",
    "\n",
    "    print(\"\\n--- IPS Constraint Checks for Optimal Portfolio ---\")\n",
    "\n",
    "    # 1. Volatility Constraint Check\n",
    "    volatility_limit = 0.15\n",
    "    print(f\"Annual Volatility (Target max {volatility_limit*100:.0f}%) : {optimal_portfolio_std*100:.2f}%\")\n",
    "    if optimal_portfolio_std < volatility_limit:\n",
    "        print(\"  Status: Respected\")\n",
    "    else:\n",
    "        print(\"  Status: Violated\")\n",
    "\n",
    "    # Calculate number of assets\n",
    "    num_assets = len(tickers)\n",
    "\n",
    "    # Create an equal_weights NumPy array\n",
    "    equal_weights = np.array([1/num_assets for _ in range(num_assets)])\n",
    "\n",
    "    # Calculate expected returns for equally weighted portfolio\n",
    "    equal_weighted_returns = expected_returns(equal_weights, log_returns)\n",
    "\n",
    "    print(\"\\n--- Expected Returns results ---\")\n",
    "\n",
    "    print(f\"\\nExpected Returns (Optimal Portfolio): {optimal_portfolio_returns:.4f}\")\n",
    "    print(f\"Expected Returns (Equally Weighted Portfolio): {equal_weighted_returns:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Historical VaR Analysis ---\")\n",
    "    # Call summarize_historical_var\n",
    "    def summarize_historical_var_updated(log_returns, optimal_weights, equal_weights, confidence_interval, portfolio_value, days):\n",
    "        historical_optimal_returns = (log_returns * optimal_weights).sum(axis=1)\n",
    "        range_optimal_returns = historical_optimal_returns.rolling(window=days).sum().dropna()\n",
    "        range_optimal_returns_dollar = range_optimal_returns * portfolio_value\n",
    "\n",
    "        historical_equal_returns = (log_returns * equal_weights).sum(axis=1)\n",
    "        range_equal_returns = historical_equal_returns.rolling(window=days).sum().dropna()\n",
    "        range_equal_returns_dollar = range_equal_returns * portfolio_value\n",
    "\n",
    "        VaR_optimized = Value_at_risk(range_optimal_returns, confidence_interval, portfolio_value)\n",
    "        VaR_equally_weighted = Value_at_risk(range_equal_returns, confidence_interval, portfolio_value)\n",
    "\n",
    "        print(f\"VaR for Optimal Portfolio: -${VaR_optimized:.2f}\")\n",
    "        print(f\"VaR for Equally Weighted Portfolio: -${VaR_equally_weighted:.2f}\")\n",
    "\n",
    "        plot_var_distribution(range_optimal_returns_dollar, VaR_optimized, confidence_interval, days, f'Distribution of Optimized Portfolio {days}-Day Returns (Dollar Value)')\n",
    "        plot_var_distribution(range_equal_returns_dollar, VaR_equally_weighted, confidence_interval, days, f'Distribution of Equally Weighted Portfolio {days}-Day Returns (Dollar Value)')\n",
    "\n",
    "        return VaR_optimized, VaR_equally_weighted, historical_optimal_returns, historical_equal_returns\n",
    "\n",
    "    VaR_optimal, VaR_equal, historical_optimal_returns, historical_equal_returns = \\\n",
    "        summarize_historical_var_updated(log_returns, optimal_weights, equal_weights, confidence_interval, portfolio_value, days)\n",
    "\n",
    "    # 3. Value at Risk Constraint Check (Historical VaR for consistency with previous checks)\n",
    "    VaR_limit_percentage = 0.10\n",
    "    VaR_percentage_optimal = VaR_optimal / portfolio_value\n",
    "    print(f\"Value at Risk (Target max {VaR_limit_percentage*100:.0f}%) : {VaR_percentage_optimal*100:.2f}%\")\n",
    "    if VaR_percentage_optimal < VaR_limit_percentage:\n",
    "        print(\"  Status: Respected\")\n",
    "    else:\n",
    "        print(\"  Status: Violated\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Maximum Drawdown Analysis ---\")\n",
    "\n",
    "    cumulative_optimal_returns = historical_optimal_returns.cumsum()\n",
    "    print(\"Optimal Portfolio:\")\n",
    "    # Calculate max drawdown for optimal portfolio here to check constraint\n",
    "    max_drawdown_optimal = summarize_max_drawdown(cumulative_optimal_returns)\n",
    "    # 2. Maximum Drawdown Constraint Check\n",
    "    max_drawdown_limit = -0.25\n",
    "    print(f\"Maximum Drawdown (Target max {max_drawdown_limit*100:.0f}%) : {max_drawdown_optimal*100:.2f}%\")\n",
    "    if max_drawdown_optimal > max_drawdown_limit: # Max drawdown is a negative value, so \">\" means less severe loss\n",
    "        print(\"  Status: Respected\")\n",
    "    else:\n",
    "        print(\"  Status: Violated\")\n",
    "\n",
    "    cumulative_equal_returns = historical_equal_returns.cumsum()\n",
    "    print(\"Equally Weighted Portfolio:\")\n",
    "    summarize_max_drawdown(cumulative_equal_returns) # This will print max drawdown for equal portfolio\n",
    "\n",
    "    print(\"\\n--- Distribution Moments ---\")\n",
    "    summarize_distribution_moments(historical_optimal_returns, historical_equal_returns)\n",
    "\n",
    "    # --- Jarque-Bera Normality Test ---\n",
    "    print(\"\\n--- Jarque-Bera Normality Test ---\")\n",
    "    jb_optimal = jarque_bera(historical_optimal_returns)\n",
    "    jb_equal = jarque_bera(historical_equal_returns)\n",
    "\n",
    "    print(\"Optimal Portfolio Returns - Jarque-Bera Test:\")\n",
    "    print(f\"  Statistic: {jb_optimal[0]:.4f}\")\n",
    "    print(f\"  p-value: {jb_optimal[1]:.4f}\")\n",
    "    if jb_optimal[1] < 0.05: # Common significance level\n",
    "        print(\"  Conclusion: Reject null hypothesis of normality (Not Normal)\\n\")\n",
    "    else:\n",
    "        print(\"  Conclusion: Fail to reject null hypothesis of normality (Normal or close to Normal)\\n\")\n",
    "\n",
    "    print(\"Equally Weighted Portfolio Returns - Jarque-Bera Test:\")\n",
    "    print(f\"  Statistic: {jb_equal[0]:.4f}\")\n",
    "    print(f\"  p-value: {jb_equal[1]:.4f}\")\n",
    "    if jb_equal[1] < 0.05:\n",
    "        print(\"  Conclusion: Reject null hypothesis of normality (Not Normal)\\n\")\n",
    "    else:\n",
    "        print(\"  Conclusion: Fail to reject null hypothesis of normality (Normal or close to Normal)\\n\")\n",
    "\n",
    "    # --- Benchmark Portfolio Calculation ---\n",
    "    print(\"\\n--- Benchmark Portfolio Analysis ---\")\n",
    "    benchmark_tickers = ['IWDA.AS', 'GLD']\n",
    "    benchmark_weights = {'IWDA.AS': 0.8, 'GLD': 0.2}\n",
    "\n",
    "    benchmark_close_df = pd.DataFrame()\n",
    "    for ticker in benchmark_tickers:\n",
    "        data = yf.download(ticker, start=startDate, end=endDate, progress=False, auto_adjust=False)\n",
    "        benchmark_close_df[ticker] = data['Adj Close']\n",
    "\n",
    "    benchmark_log_returns = np.log(benchmark_close_df / benchmark_close_df.shift(1)).dropna()\n",
    "\n",
    "    # Align benchmark returns with the existing log_returns index\n",
    "    common_index = log_returns.index.intersection(benchmark_log_returns.index)\n",
    "    aligned_benchmark_log_returns = benchmark_log_returns.reindex(common_index)\n",
    "\n",
    "    # Calculate historical weighted returns for the benchmark\n",
    "    historical_benchmark_returns = (\n",
    "        aligned_benchmark_log_returns['IWDA.AS'] * benchmark_weights['IWDA.AS']\n",
    "    ) + (\n",
    "        aligned_benchmark_log_returns['GLD'] * benchmark_weights['GLD']\n",
    "    )\n",
    "\n",
    "    cumulative_benchmark_returns = historical_benchmark_returns.cumsum()\n",
    "    print(f\"Cumulative returns for Benchmark Portfolio (80% MSCI World, 20% Gold):\\n{cumulative_benchmark_returns.iloc[-1]:.4f}\")\n",
    "\n",
    "    # Recalculate benchmark portfolio std for efficient frontier plot\n",
    "    # Create a NumPy array for benchmark weights in the correct order based on aligned_benchmark_log_returns columns\n",
    "    benchmark_weights_array = np.array([benchmark_weights[t] for t in aligned_benchmark_log_returns.columns])\n",
    "    benchmark_portfolio_returns = expected_returns(benchmark_weights_array, aligned_benchmark_log_returns)\n",
    "    benchmark_portfolio_std = standard_dev(benchmark_weights_array, aligned_benchmark_log_returns.cov() * 252)\n",
    "\n",
    "\n",
    "    # --- Plotting Cumulative Returns ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_optimal_returns.index, cumulative_optimal_returns, label='Optimal Portfolio Cumulative Returns', color='blue')\n",
    "    plt.plot(cumulative_equal_returns.index, cumulative_equal_returns, label='Equally Weighted Portfolio Cumulative Returns', color='green')\n",
    "    plt.plot(cumulative_benchmark_returns.index, cumulative_benchmark_returns, label='Benchmark Portfolio Cumulative Returns (80% IWDA.AS, 20% GLD)', color='red')\n",
    "    plt.title('Cumulative Returns Comparison')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Log Returns')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Efficient Frontier Analysis ---\")\n",
    "\n",
    "    # Define function for simulating random portfolios\n",
    "    def simulate_random_portfolios(num_portfolios, log_returns, cov_matrix_opt, rf):\n",
    "        portfolio_returns = []\n",
    "        portfolio_volatilities = []\n",
    "        portfolio_sharpe_ratios = []\n",
    "\n",
    "        num_assets = len(log_returns.columns)\n",
    "\n",
    "        for _ in range(num_portfolios):\n",
    "            weights = np.random.random(num_assets)\n",
    "            weights /= np.sum(weights) # Normalize weights to sum to 1\n",
    "\n",
    "            # Implement allocation constraints: max weight 0.30 per asset\n",
    "            # Regenerate if any weight exceeds 0.30\n",
    "            # This is a simple approach, more sophisticated methods could be used\n",
    "            while np.any(weights > 0.30):\n",
    "                weights = np.random.random(num_assets)\n",
    "                weights /= np.sum(weights)\n",
    "\n",
    "            # Calculate portfolio metrics\n",
    "            ret = expected_returns(weights, log_returns)\n",
    "            vol = standard_dev(weights, cov_matrix_opt)\n",
    "            sharpe = Sharpe_ratio(weights, log_returns, cov_matrix_opt, rf)\n",
    "\n",
    "            portfolio_returns.append(ret)\n",
    "            portfolio_volatilities.append(vol)\n",
    "            portfolio_sharpe_ratios.append(sharpe)\n",
    "\n",
    "        return np.array(portfolio_returns), np.array(portfolio_volatilities), np.array(portfolio_sharpe_ratios)\n",
    "\n",
    "    # Define function for plotting the efficient frontier\n",
    "    def plot_efficient_frontier(random_volatilities, random_returns, random_sharpe_ratios,\n",
    "                               optimal_volatility, optimal_return,\n",
    "                               benchmark_volatility, benchmark_return):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Plot random portfolios, colored by Sharpe Ratio\n",
    "        scatter = plt.scatter(random_volatilities, random_returns, c=random_sharpe_ratios, cmap='viridis', s=10, alpha=0.6, label='Random Portfolios')\n",
    "        plt.colorbar(scatter, label='Sharpe Ratio')\n",
    "\n",
    "        # Plot optimal portfolio\n",
    "        plt.scatter(optimal_volatility, optimal_return, color='red', marker='*', s=300, label='Optimal Portfolio (Max Sharpe Ratio)')\n",
    "\n",
    "        # Plot benchmark portfolio\n",
    "        plt.scatter(benchmark_volatility, benchmark_return, color='purple', marker='X', s=200, label='Benchmark Portfolio')\n",
    "\n",
    "        plt.title('Efficient Frontier with Random, Optimal, and Benchmark Portfolios')\n",
    "        plt.xlabel('Annualized Volatility (Standard Deviation)')\n",
    "        plt.ylabel('Annualized Expected Return')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    num_portfolios = 10000\n",
    "    random_port_returns, random_port_volatilities, random_port_sharpe_ratios = simulate_random_portfolios(\n",
    "        num_portfolios, log_returns, cov_matrix_opt, rf\n",
    "    )\n",
    "\n",
    "    plot_efficient_frontier(random_port_volatilities, random_port_returns, random_port_sharpe_ratios,\n",
    "                            optimal_portfolio_std, optimal_portfolio_returns,\n",
    "                            benchmark_portfolio_std, benchmark_portfolio_returns)\n",
    "\n",
    "    print(\"\\n--- Monte Carlo Simulation (Bootstrap) ---\")\n",
    "\n",
    "    # Get number of historical observations\n",
    "    n_historical_days = len(log_returns)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # --- Optimal Portfolio Simulation ---\n",
    "    simulated_optimal_portfolio_values = np.zeros((num_days, num_simulations))\n",
    "    for s in range(num_simulations):\n",
    "        # Bootstrap: randomly select num_days rows from historical log_returns with replacement\n",
    "        random_indices = np.random.choice(n_historical_days, num_days, replace=True)\n",
    "        bootstrapped_daily_returns = log_returns.iloc[random_indices]\n",
    "\n",
    "        # Calculate portfolio daily returns using bootstrapped returns and optimal weights\n",
    "        portfolio_daily_returns_optimal = np.dot(bootstrapped_daily_returns, optimal_weights)\n",
    "        simulated_optimal_portfolio_values[:, s] = portfolio_value * np.exp(np.cumsum(portfolio_daily_returns_optimal))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(simulated_optimal_portfolio_values)\n",
    "    plt.title(f'Optimal Portfolio Monte Carlo Simulation (Bootstrap, {num_simulations} runs)')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- Equally Weighted Portfolio Simulation ---\n",
    "    simulated_equal_portfolio_values = np.zeros((num_days, num_simulations))\n",
    "    for s in range(num_simulations):\n",
    "        # Bootstrap: randomly select num_days rows from historical log_returns with replacement\n",
    "        random_indices = np.random.choice(n_historical_days, num_days, replace=True)\n",
    "        bootstrapped_daily_returns = log_returns.iloc[random_indices]\n",
    "\n",
    "        # Calculate portfolio daily returns using bootstrapped returns and equal weights\n",
    "        portfolio_daily_returns_equal = np.dot(bootstrapped_daily_returns, equal_weights)\n",
    "        simulated_equal_portfolio_values[:, s] = portfolio_value * np.exp(np.cumsum(portfolio_daily_returns_equal))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(simulated_equal_portfolio_values)\n",
    "    plt.title(f'Equally Weighted Portfolio Monte Carlo Simulation (Bootstrap, {num_simulations} runs)')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Summary Statistics for Final Portfolio Values\n",
    "    final_optimal_values = simulated_optimal_portfolio_values[-1, :]\n",
    "    final_equal_values = simulated_equal_portfolio_values[-1, :]\n",
    "\n",
    "    print(\"\\n--- Summary Statistics for Final Portfolio Values ---\")\n",
    "    print(\"Optimal Portfolio:\")\n",
    "    print(f\"  Mean: ${np.mean(final_optimal_values):.2f}\")\n",
    "    print(f\"  Median: ${np.median(final_optimal_values):.2f}\")\n",
    "    print(f\"  Standard Deviation: ${np.std(final_optimal_values):.2f}\")\n",
    "    print(f\"  Minimum: ${np.min(final_optimal_values):.2f}\")\n",
    "    print(f\"  Maximum: ${np.max(final_optimal_values):.2f}\")\n",
    "\n",
    "    print(\"\\nEqually Weighted Portfolio:\")\n",
    "    print(f\"  Mean: ${np.mean(final_equal_values):.2f}\")\n",
    "    print(f\"  Median: ${np.median(final_equal_values):.2f}\")\n",
    "    print(f\"  Standard Deviation: ${np.std(final_equal_values):.2f}\")\n",
    "    print(f\"  Minimum: ${np.min(final_equal_values):.2f}\")\n",
    "    print(f\"  Maximum: ${np.max(final_equal_values):.2f}\")\n",
    "\n",
    "    print(\"\\n--- Comprehensive Portfolio Analysis Complete ---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_for_analysis = top_15_pf.index.tolist()\n",
    "summarize_all_portfolio_analysis(tickers_for_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_optimal_pie(tickers, optimal_weights):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.pie(optimal_weights, labels=tickers, autopct='%1.1f%%', startangle=90,\n",
    "            colors=plt.cm.tab20.colors)\n",
    "    plt.title(\"Optimal Portfolio Allocation\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_optimal_pie(tickers, optimal_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37649eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "corr_matrix = returns_pf.corr()\n",
    "\n",
    "# Affichage de la corrélation de l'Or avec les autres actifs\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix[['GLD']].sort_values(by='GLD', ascending=False),\n",
    "            annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Corrélation de l'or avec les actifs du portefeuille\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tickers = ['GE', 'MCK', 'AVGO', 'CEG', 'APH', 'LLY', 'IBM', 'PLTR',\n",
    "           'WMT', 'TRGP', 'NVDA', 'TJX', 'GS', 'WELL', 'WMB', 'GLD']\n",
    "\n",
    "optimal_weights = np.array([0.0930, 0.2459, 0.0477, 0.0120, 0.0, 0.0945, 0.0036, 0.0129,\n",
    "                            0.1285, 0.0285, 0.0039, 0.0, 0.0, 0.0209, 0.0087, 0.3000])\n",
    "\n",
    "equal_weights = np.array([1/len(tickers) for _ in tickers])\n",
    "\n",
    "days = 10\n",
    "np.random.seed(42)\n",
    "cov_matrix = log_returns.cov().values\n",
    "\n",
    "stress_cov = cov_matrix.copy()\n",
    "\n",
    "\n",
    "stress_cov[:-1, :-1] *= 3\n",
    "\n",
    "\n",
    "mu_shock = np.array([-0.025]*(len(tickers)-1) + [0.002])\n",
    "# -2.5% / jour actions (~ -25% en 10j)\n",
    "\n",
    "shock_returns = np.random.multivariate_normal(\n",
    "    mean=mu_shock,\n",
    "    cov=stress_cov,\n",
    "    size=days\n",
    ")\n",
    "\n",
    "log_returns_shock = pd.DataFrame(shock_returns, columns=tickers)\n",
    "\n",
    "def cumulative_portfolio_returns(weights, returns):\n",
    "    return (returns @ weights).cumsum()\n",
    "\n",
    "cum_optimal = cumulative_portfolio_returns(optimal_weights, log_returns_shock)\n",
    "cum_equal = cumulative_portfolio_returns(equal_weights, log_returns_shock)\n",
    "cum_gld = log_returns_shock['GLD'].cumsum()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cum_optimal, label='Optimal Portfolio', linewidth=2)\n",
    "plt.plot(cum_equal, label='Equally Weighted Portfolio', linewidth=2)\n",
    "plt.plot(cum_gld, label='GLD Benchmark', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('Stress Test Crash 2008-like')\n",
    "plt.xlabel('Jours')\n",
    "plt.ylabel('Rendements cumulés (log)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "def portfolio_stats(cum_returns, name):\n",
    "    total_return = cum_returns.iloc[-1]\n",
    "    max_drawdown = (cum_returns - cum_returns.cummax()).min()\n",
    "    vol = cum_returns.diff().std() * np.sqrt(252)\n",
    "\n",
    "    print(f\"{name}\")\n",
    "    print(f\"Total Return: {total_return:.4f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.4f}\")\n",
    "    print(f\"Volatility: {vol:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"=== Résultats Stress Test Crash 2008-like ===\")\n",
    "portfolio_stats(cum_optimal, \"Optimal Portfolio\")\n",
    "portfolio_stats(cum_equal, \"Equally Weighted Portfolio\")\n",
    "portfolio_stats(cum_gld, \"GLD Benchmark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab426174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "tickers = ['GE', 'MCK', 'AVGO', 'CEG', 'APH', 'LLY', 'IBM', 'PLTR',\n",
    "           'WMT', 'TRGP', 'NVDA', 'TJX', 'GS', 'WELL', 'WMB', 'GLD']\n",
    "\n",
    "optimal_weights_before = np.array([0.0930, 0.2459, 0.0477, 0.0120, 0.0, 0.0945, 0.0036, 0.0129,\n",
    "                                   0.1285, 0.0285, 0.0039, 0.0, 0.0, 0.0209, 0.0087, 0.3000])\n",
    "\n",
    "# Téléchargement des données\n",
    "endDate = pd.Timestamp.today()\n",
    "startDate = endDate - pd.Timedelta(days=365*5)\n",
    "data = yf.download(tickers, start=startDate, end=endDate, auto_adjust=True, progress=False)['Close']\n",
    "log_returns = np.log(data / data.shift(1)).dropna()\n",
    "\n",
    "\n",
    "window = 42  # 2 mois\n",
    "portfolio_returns = log_returns.dot(optimal_weights_before)\n",
    "\n",
    "features = pd.DataFrame(index=log_returns.index)\n",
    "features['MA'] = portfolio_returns.rolling(window).mean()\n",
    "features['Volatility'] = portfolio_returns.rolling(window).std()\n",
    "features['NegativeRatio'] = (portfolio_returns.rolling(window).apply(lambda x: (x<0).sum())/window)\n",
    "features['Market'] = np.where(features['NegativeRatio']>0.5, 0, 1)  # 0 = baissier, 1 = haussier\n",
    "features = features.dropna()\n",
    "\n",
    "X = features[['MA','Volatility','NegativeRatio']]\n",
    "y = features['Market']\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "signal = rf.predict(X_scaled[-1].reshape(1,-1))[0]\n",
    "market_condition = 'Baissier' if signal==0 else 'Haussier'\n",
    "print(f\"Signal ML détecté : {market_condition}\")\n",
    "\n",
    "\n",
    "cov_matrix = log_returns.cov() * 252\n",
    "expected_returns = log_returns.mean() * 252\n",
    "rf_rate = 0.03447  # taux sans risque\n",
    "\n",
    "def markowitz_sharpe(weights, expected_returns, cov_matrix, rf=rf_rate):\n",
    "    port_return = np.dot(weights, expected_returns)\n",
    "    port_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    sharpe = (port_return - rf) / port_vol\n",
    "    return -sharpe\n",
    "\n",
    "constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x)-1})\n",
    "bounds = [(0,0.3) for _ in tickers]\n",
    "\n",
    "\n",
    "init_weights = optimal_weights_before.copy()\n",
    "if market_condition=='Baissier':\n",
    "    gl_index = tickers.index('GLD')\n",
    "    init_weights[gl_index] += 0.1\n",
    "    other_indices = [i for i in range(len(tickers)) if i != gl_index]\n",
    "    init_weights[other_indices] *= (1 - init_weights[gl_index])/sum(init_weights[other_indices])\n",
    "\n",
    "res = minimize(markowitz_sharpe, init_weights, args=(expected_returns.values, cov_matrix.values),\n",
    "               method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "adjusted_weights = res.x\n",
    "\n",
    "\n",
    "def portfolio_metrics(weights, log_returns, rf=rf_rate):\n",
    "    port_ret = np.sum(log_returns.mean() * weights) * 252\n",
    "    port_vol = np.sqrt(np.dot(weights.T, np.dot(log_returns.cov()*252, weights)))\n",
    "    sharpe = (port_ret - rf)/port_vol\n",
    "    return port_ret, port_vol, sharpe\n",
    "\n",
    "ret_before, vol_before, sharpe_before = portfolio_metrics(optimal_weights_before, log_returns)\n",
    "ret_after, vol_after, sharpe_after = portfolio_metrics(adjusted_weights, log_returns)\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Metric': ['Expected Return','Volatility','Sharpe Ratio'],\n",
    "    'Before ML+Markowitz':[ret_before, vol_before, sharpe_before],\n",
    "    'After ML+Markowitz':[ret_after, vol_after, sharpe_after]\n",
    "})\n",
    "print(\"\\n--- Rendements & Risques Avant/Après ---\")\n",
    "print(df_metrics)\n",
    "\n",
    "\n",
    "df_weights = pd.DataFrame({\n",
    "    'Ticker': tickers,\n",
    "    'Before ML': optimal_weights_before,\n",
    "    'After ML': adjusted_weights\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(tickers))\n",
    "plt.bar(x - bar_width/2, optimal_weights_before, width=bar_width, label='Before ML', color='skyblue')\n",
    "plt.bar(x + bar_width/2, adjusted_weights, width=bar_width, label='After ML', color='orange')\n",
    "plt.xticks(x, tickers, rotation=45, ha='right')\n",
    "plt.ylabel(\"Poids dans le portefeuille\")\n",
    "plt.title(\"Portefeuille Avant / Après ML\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
